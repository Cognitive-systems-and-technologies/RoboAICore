# RoboAI Core Library
Кроссплатформенная библиотека глубокого обучения, для мобильных роботов и ПК. Библиотека может применяться в направлениях, где требуется разработка систем, использующих нейронные сети, модели глубокого машинного обучения, обучение с подкреплением интеллектуальных агентов. 

Основные направления прикладного использования библиотеки:
- образовательная робототехника;
- мобильные роботы, машинки, манипуляторы и т.п.,
- устройства систем умного дома.

Для иллюстрации работы алгоритмов глубокого обучения, представленных в библиотеке показана задача обучения мобильного робота на транспортной платформе Yahboom Raspbot с установленными на роботе ультразвуковыми дальномерами для определения расстояний до объектов типа HC-SR04. На робот также установлена отладочная плата [32f429 discovery](https://www.st.com/en/evaluation-tools/32f429idiscovery.html) под управлением которой он работает.

Пример обучения агента на базе stm32f429. Обучение происходит на аппаратной части stm32f429, используя алгоритм DeepRl реализованный в библиотеке. Данные о процессе обучения передаются при помощи модуля esp8266 в виде http-сообщений на серверную часть для мониторинга. Мониторинг и управление агентом осуществляется при помощи специально разработанного веб-интерфейса [NeuralInterface](https://github.com/Cognitive-systems-and-technologies/NeuralInterface)

Задача самообучения робота формулируется следующим образом:
Входной вектор (вектор состояния S) принимает значения с трех сенсоров HC-SR04 и имеет вид: [d1, d2, d3], где d – дистанция до препятствий. Выход искусственной нейронной сети – вектор [a1, a2, a3], где а – оценка одного из трех действий, которые может выполнять агент. Действия a1-ехать прямо, a2-повернуть налево, a3-повернуть направо. Функция поощрения имеет следующий вид:
```
float GetReward(Eyes *eyes, int action, float max_length)
{
	float proximity_reward = 0.0f;
	proximity_reward += eyes->distVec[0] / max_length;
	proximity_reward += eyes->distVec[1] / max_length;
	proximity_reward += eyes->distVec[2] / max_length;
	proximity_reward = proximity_reward / 3.f;
	float forwardReward = 0;
	if (action == 0 && proximity_reward > 0.8f) forwardReward = 0.5f;
	float res = proximity_reward + forwardReward;
	return res;
}
```
Если нет препятствий, и выполняется действие a1 - агент получает максимальное поощрение, в противном случае агент получает поощрение в зависимости от дистанции до препятствий, чем она меньше, тем меньше поощрение. Также агент получает дополнительное поощрение при выходе за пределы ограниченной препятствиями зоны. Все структуры и алгоритмы обучения выполняются на STM32F429. На локальный ПК передаются только данные для мониторинга и сигналы управления агентом. Для обеспечения связи между агентом и сервером был использован модуль ESP8266, на котором реализован http-клиент для отправки запросов на глобальный сервер и локальный web-сервер для обработки входящих запросов. Робот считается обученным, когда в процессе определенного количества итераций значение ошибки обучения меньше 1.f и агент может объезжать препятствия, двигаясь в сторону выхода из ограниченной препятствиями зоны.

Пример работы обученного агента:

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/ea0d8646-0c95-4f4a-b4c9-754df7526ee1

Пример процесса обучения агента:

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/12a71c56-2717-4aa6-8c5e-a0d0802e9ae2

Пример компиляции программы для stm32f429:

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/9dda2685-c922-4ba5-9f9c-bd272eaf76bc

В качестве тестирования алгоритмов обучения с подкреплением был выбран пример с поиском кратчайших путей к цели в лабиринте (алгоритм qmaze). Допустим, у нас есть лабиринт 10X10 ячеек, часть ячеек свободны, часть из них закрыты - это стенки лабиринта. В лабиринт помещается агент, которому нужно за минимальное количество шагов добраться до целевой ячейки. Агент получает небольшой штраф за каждый ход по свободной ячейке и больший штраф за попытку хода на закрытую ячейку. Причина такого отрицательного наказания в том, что мы хотим, чтобы агент попал в целевую ячейку по кратчайшему пути и не врезался в стены. За переход в целевую ячейку агент получает максимальное поощрение. Агент может передвигаться только по свободным клеткам, основная цель агента - добраться до целевой ячейки. 

Пример работы алгоритма qmaze реализованного с использованием библиотеки RoboAICore:

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/303822bc-dc65-43bc-8983-5437ec26f108

После некоторого количества эпизодов обучения агент научился определять кратчайшее расстояние до целевой ячейки из подавляющего числа стартовых позиций.

Для raspberry pi была протестирована модель сверточной нейросети для распознавания объектов. В качестве примера распознавания было выбрана задача распознавания одним агентом другого, когда тот попадает в поле зрения камеры. Для обучения был составлен небольшой датасет на 2 класса: комната и агент. Вход сети имеет размерность 227x227x3. Всего при обучении было использовано 40 изображений по 20 на каждый класс.

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/adc06d2f-c17e-45cb-9943-d3c8b28b5788

### Функции:
- Нейронные сети прямого распространения,
- Свёрточные нейронные сети,
- Алгоритмы обучения с подкреплением (deep RL)

### Адгоритмы оптимизации:
- SGD - метод простого стохастического градиентного спуска (stochastic gradient descent),
- Adagrad - метод адаптивного градиентного спуска (adaptive gradient algorithm),
- RMSProp - метод модифицированного адаптивного градиентного спуска (root mean square propagation),
- Nesterov - алгоритм Нестерова, метод накопления импульса (Nesterov Accelerated Gradient),
- Adam - метод адаптивной оценки момента (Adaptive Moment Estimation),
- Adan - адаптивный алгоритм импульса Нестерова (Adaptive Nesterov Momentum Algorithm).

## Сборка и компиляция:
Проект был протестирован на: STM32f429, RaspberryPi 3 model B и персональном компьютере.

Требования для работы на микроконтроллерах:
- В библиотеке используется тип данных float, поэтому необходима поддержка микроконтроллером вычислений чисел с плавающей точкой (FPU). Для stm32 это, например, STM32F4xx, STM32F74x/5x, STM32L4xx, STM32F76x/7x и т.п.
- Объем оперативной памяти, требуемой для работы, зависит от размеров создаваемой модели и может варьироваться от нескольких десятков килобайт и до размеров доступной на устройстве памяти.
- Размер программируемой памяти от 500Кб и выше.
- Поддержка компилятором и наличие стандартных библиотек С (libc).

Проект использует систему сборки CMake.
Для сборки и компиляции проекта на ПК необходимо наличие установленной программы cmake и компилятора.

Для компиляции библиотеки с поддержкой вычислений на GPU необходимо установить инструментальные средства разработки для создания приложений для архитектуры CUDA - “NVIDIA GPU Computing Toolkit”. Загрузить пакет для установки можно на сайте nvidia по ссылке:
```
https://developer.nvidia.com/cuda-downloads
```
Поддерживаются графические процессоры с версией архитектуры compute_60 (Pascal) и выше.

Далее представлен процесс сборки и компиляции для операционной системы Windows на примере использования cmake-gui + Visual Studio MSVC:
- скопируйте репозиторий
```bash
 $ git clone --recursive https://github.com/Cognitive-systems-and-technologies/RoboAICore.git
```
- откройте cmake и укажите пути к папке с проектом и папке, в которую будет собран проект, затем нажмите "configure" и выберете тип проекта и компилятор (для VS можно оставить по умолчанию),
- после завершения конфигурации нажмите "generate" и "open project". Проект откроется в VIsual Studio,
- в окне Visual Studio выберите тип компиляции и в меню "Build"->"Rebuild solution".

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/3f293dad-f79e-4311-9cd7-68e277f902dc

Далее представлен процесс сборки на RaspberryPi для операционной системы Debian Linux:
- скопируйте репозиторий
- откройте терминал и выполните команды для обновления и установки cmake
```
sudo apt update
sudo apt install -y cmake
```
- перейдите в папку с исходным кодом и запустите командную строку из дирректории
- выполните следующие команды для сборки проекта и компиляции:
```
cmake .
make
```

https://github.com/Cognitive-systems-and-technologies/RoboAICore/assets/100981393/a33fce38-b1c1-4e9d-a61a-3d87e6776f97

После компиляции будут созданы исполняемые файлы с примерами создания и обучения моделей. Файлы с кодом примеров расположены в папке cmd:
[примеры](https://github.com/Cognitive-systems-and-technologies/RoboAICore/tree/main/src/cmd). Список примеров:
- cuda_test.cu - пример создания и обучения модели из трех полносвязных слоев и функциями активации гиперболического тангенса на GPU,
- rand_test.cpp - пример работы алгоритмов для инициализации весовых коэффициентов,
- opt_test.cpp - пример создания и обучения модели из трех полносвязных слоев и функциями активации гиперболического тангенса на CPU,
- mult_opt_test.cpp - пример создания и обучения модели с несколькими выходами на CPU,
- model_test.cpp - пример создания глубокой модели нейросети (по типу AlexNet) на CPU,
- data_test.cpp - пример работы с данными и функциями слоев.

## Ресурсы:
- Описание функций библиотеки [RoboAICore API](https://github.com/Cognitive-systems-and-technologies/materials/blob/main/RAI_API.pdf).
